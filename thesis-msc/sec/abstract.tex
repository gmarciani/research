% %
% MOTIVATION
% %
The orchestration of containerized micro-service applications in a Cloud environment provides technicians with a fine-grained control on how software components scale and are placed over a theoretically infinite pool of distributed resources.
Providing elasticity to such orchestration is considered a strategic asset for Cloud-based industries, because scaling an placement strategies may have a relevant impact on both application performances perceived by the end-user and the overall cost of the underlying infrastructure.
%
% %
% PROBLEM STATEMENT
% %
Given the high variability of workloads, manual tuning is clearly impractical and even automated solutions leveraging static threshold based policies may be ineffective in satisfying QoS expectations, SLAs and budget requirements.
In this context, it's increasingly important to investigate solutions based on self-adaptive strategies, opening the way for new research fields and industrial solutions.

% %
% APPROACH
% %
In this work, we propose and compare some heuristics for the elastic containers orchestration, leveraging Reinforcement Learning.
%
In particular, we focus on solutions where scale-in and scale-out decisions are driven by multi-agent multi-metric self-adaptive thresholds.
%
We evaluate the proposed solutions using a simulator that has been designed specifically to investigate scaling and placement strategies for containerized applications in a Cloud environment.

%
%
% %
% RESULTS
% %
The experimental results show that solution [X] converges to an auto-scaling policy that meets QoS requirements while reducing the cost of the infrastructure, with a training period of [X] hours.
%
%
% %
% CONCLUSIONS
% %
Although there is still a long way to go to speed-up the learning convergence, our work shows that the application of Reinforcement Learning for elastic containers auto-scaling is a promising solution for Cloud-driven industries.
