% This file was created with JabRef 2.10.
% Encoding: UTF-8


@Article{Ozkose2015,
  Title                    = {{Yesterday, Today and Tomorrow of Big Data}},
  Author                   = {{\"{O}}zk{\"{o}}se, Hakan and Ari, Emin Sertac and Gencer, Cevriye},
  Journal                  = {Procedia - Social and Behavioral Sciences},
  Year                     = {2015},

  Month                    = {jul},
  Pages                    = {1042--1050},
  Volume                   = {195},

  Abstract                 = {Owing to the self-improvement desire, the human being always tries to reach to the current information and generate new ones from the data on hand. The practices are realized by processing and transforming the data, whose existence is broadly accepted, into information. Generating information from data is vitally important in terms of regulating the life. Especially firms need to store and transform data quickly and properly into information in order to achieve the objectives such as having a competitive edge, producing new products, moving the firm ahead and stabilizing the internal dynamics. The increase in the amount of data sources also increases the amount of the data acquired. Therefore storing and processing data become difficult and classical approaches remain incapable to do such transactions. By means of Big Data large amount of data with a wide range can be stored, managed and processed. Besides Big Data ensures proper information quickly and offers advantage and convenience to the firms, researchers and consumers by taking the properties of Volume, Value, Variety, Veracity and Velocity into consideration. This study consists of 5 parts. In the Introduction part the features, classification, the process, the areas of usage and the techniques of Big Data are explained. In the second part the appearance process and the advantages of the concept of Big Data are illustrated with examples. A detailed literature review is produced in the third part. The actual studies and the most interested areas of Big Data are told in this part. In the fourth part the future of the Big Data is evaluated. Besides the situation and distribution of the studies on Big Data in Turkey and all over the world is presented. In the Conclusion part, an overall assessment is included and probable troubles are mentioned.},
  Annote                   = {Figura2: distribuzione temporale dell'interesse per i Big Data
Figura3: distribuzione temporale dell'interesse per Big Data vs Data Mining
Figura6: distribuzione geografica del'interesse per i Big Data},
  Doi                      = {10.1016/j.sbspro.2015.06.147},
  File                     = {:home/giacomo/Documents/research/mendeley library/Yesterday, Today and Tomorrow of Big Data. 2015. {\"{O}}zk{\"{o}}se, Ari, Gencer.pdf:pdf},
  ISSN                     = {18770428},
  Keywords                 = {Big data,data,information},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877042815036265}
}

@Misc{AkamaiConnectivityVisualization,
  Title                    = {{Connectivity Visualizations}},

  Author                   = {Akamai},
  HowPublished             = {http://bit.ly/akamai-connectivity-visualizations},
  Year                     = {2015},

  Booktitle                = {State of the Internet},
  Institution              = {Akamai},
  Keywords                 = {akamai,connectivity,visualization},
  Mendeley-tags            = {akamai,connectivity,visualization},
  Url                      = {http://bit.ly/akamai-connectivity-visualizations},
  Urldate                  = {2015-10-20}
}

@TechReport{AkamaiSOIQ22015,
  Title                    = {{State of the Internet}},
  Author                   = {Akamai},
  Institution              = {Akamai},
  Year                     = {2015},

  Abstract                 = {akamai; internet},
  File                     = {:home/giacomo/Documents/research/mendeley library/State of the Internet. 2015. Unknown.pdf:pdf},
  Url                      = {http://bit.ly/Akamai-State-Internet-Q2-2015}
}

@InProceedings{Aniello2013,
  Title                    = {{Adaptive online scheduling in storm}},
  Author                   = {Aniello, Leonardo and Baldoni, Roberto and Querzoni, Leonardo},
  Booktitle                = {Proceedings of the 7th ACM international conference on Distributed event-based systems - DEBS '13},
  Year                     = {2013},

  Address                  = {New York, New York, USA},
  Organization             = {ACM},
  Pages                    = {207},
  Publisher                = {ACM Press},

  Abstract                 = {Today we are witnessing a dramatic shift toward a data-driven economy, where the ability to efficiently and timely analyze huge amounts of data marks the difference between industrial success stories and catastrophic failures. In this scenario Storm, an open source distributed realtime computation system, represents a disruptive technology that is quickly gaining the favor of big players like Twitter and Groupon. A Storm application is modeled as a topology, i.e. a graph where nodes are operators and edges represent data flows among such operators. A key aspect in tuning Storm performance lies in the strategy used to deploy a topology, i.e. how Storm schedules the execution of each topology component on the available computing infrastructure. In this paper we propose two advanced generic schedulers for Storm that provide improved performance for a wide range of application topologies. The first scheduler works offline by analyzing the topology structure and adapting the deployment to it; the second scheduler enhance the previous approach by continuously monitoring system performance and rescheduling the deployment at run-time to improve overall performance. Experimental results show that these algorithms can produce schedules that achieve significantly better performances compared to those produced by Storm’s default scheduler.},
  Doi                      = {10.1145/2488222.2488267},
  File                     = {:home/giacomo/Documents/research/mendeley library/Adaptive online scheduling in storm. 2013. Aniello, Baldoni, Querzoni.pdf:pdf},
  ISBN                     = {9781450317580},
  Keywords                 = {cep,distributed event processing,distributed systems,scheduling,storm},
  Mendeley-tags            = {distributed systems},
  Url                      = {http://dl.acm.org/citation.cfm?doid=2488222.2488267}
}

@Article{Anuradha2015,
  Title                    = {{A Brief Introduction on Big Data 5Vs Characteristics and Hadoop Technology}},
  Author                   = {Anuradha, J.},
  Journal                  = {Procedia Computer Science},
  Year                     = {2015},
  Pages                    = {319--324},
  Volume                   = {48},

  Abstract                 = {Big data is a collection of massive and complex data sets and data volume that include the huge quantities of data, data management capabilities, social media analytics and real-time data. Big data analytics is the process of examining large amounts of data. There exist large amounts of heterogeneous digital data. Big data is about data volume and large data set's measured in terms of terabytes or petabytes. This phenomenon is called Bigdata. After examining of Bigdata, the data has been launched as Big Data analytics. In this paper, presenting the 5Vs characteristics of big data and the technique and technology used to handle big data. The challenges include capturing, analysis, storage, searching, sharing, visualization, transferring and privacy violations. It can neither be worked upon by using traditional SQL queries nor can the relational database management system (RDBMS) be used for storage. Though, a wide variety of scalable database tools and techniques has evolved. Hadoop is an open source distributed data processing is one of the prominent and well known solutions. The NoSQL has a non-relational database with the likes of MongoDB from Apache.},
  Doi                      = {10.1016/j.procs.2015.04.188},
  File                     = {:home/giacomo/Documents/research/mendeley library/A Brief Introduction on Big Data 5Vs Characteristics and Hadoop Technology. 2015. Anuradha.pdf:pdf},
  ISSN                     = {18770509},
  Keywords                 = {Big Data.,NoSQL,RDBMS},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877050915006973}
}

@Article{Assuncao2014,
  Title                    = {{Big Data computing and clouds: Trends and future directions}},
  Author                   = {Assun{\c{c}}{\~{a}}o, Marcos D. and Calheiros, Rodrigo N. and Bianchi, Silvia and a.S. Netto, Marco and Buyya, Rajkumar},
  Journal                  = {Journal of Parallel and Distributed Computing},
  Year                     = {2014},
  Pages                    = {3--15},
  Volume                   = {79-80},

  Abstract                 = {This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions.},
  Doi                      = {10.1016/j.jpdc.2014.08.003},
  File                     = {:home/giacomo/Documents/research/mendeley library/Big Data computing and clouds Trends and future directions. 2014. Assuncao et al.pdf:pdf},
  ISSN                     = {07437315},
  Publisher                = {Elsevier Inc.},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/S0743731514001452}
}

@Article{Babcock2002,
  Title                    = {{Models and issues in data stream systems}},
  Author                   = {Babcock, Brian and Babu, Shivnath and Datar, Mayur and Motwani, Rajeev and Widom, Jennifer},
  Journal                  = {Proceedings of the twentyfirst ACM SIGMODSIGACTSIGART symposium on Principles of database systems PODS 02},
  Year                     = {2002},
  Number                   = {2002-19},
  Pages                    = {1},
  Volume                   = {pages},

  Abstract                 = {In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues.},
  Doi                      = {10.1145/543614.543615},
  File                     = {:home/giacomo/Documents/research/mendeley library/Models and issues in data stream systems. 2002. Babcock et al.pdf:pdf},
  ISBN                     = {1581135076},
  ISSN                     = {1581135076},
  Pmid                     = {20033067},
  Url                      = {http://portal.acm.org/citation.cfm?doid=543613.543615}
}

@TechReport{Bange2015,
  Title                    = {{Big Data Use Cases}},
  Author                   = {Bange, Carsten and Grosser, Timm and Janoschek, Nikolai},
  Institution              = {BARC},
  Year                     = {2015},

  File                     = {:home/giacomo/Documents/research/mendeley library/Big Data Use Cases. 2015. Bange, Grosser, Janoschek.pdf:pdf},
  Pages                    = {51}
}

@Article{Berman2013,
  Title                    = {{Introduction}},
  Author                   = {Berman, Jules J.},
  Journal                  = {Principles of Big Data},
  Year                     = {2013},
  Pages                    = {xix--xxvi},

  Doi                      = {10.1016/B978-0-12-404576-7.09980-9},
  File                     = {:home/giacomo/Documents/research/mendeley library/Introduction. 2013. Berman.pdf:pdf},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/B9780124045767099809}
}

@PhdThesis{Cardellini2014,
  Title                    = {{Un approccio network aware per il data stream processing}},
  Author                   = {Cardellini, Valeria and Nardelli, Matteo},
  School                   = {University of Rome Tor Vergata},
  Year                     = {2014},

  Address                  = {Rome, Italy},
  Type                     = {MSc in Computer Engineering},

  File                     = {:home/giacomo/Documents/research/mendeley library/Un approccio network aware per il data stream processing. 2014. Cardellini, Nardelli.pdf:pdf},
  Pages                    = {114}
}

@Misc{CERNDSP,
  Title                    = {{Animation shows LHC data processing}},

  Author                   = {CERN},
  HowPublished             = {http://bit.ly/cern-lhc-dsp},
  Year                     = {2015},

  Institution              = {CERN},
  Url                      = {http://bit.ly/cern-lhc-dsp}
}

@Misc{CERNLHC,
  Title                    = {{The Large Hadron Collider}},

  Author                   = {CERN},
  HowPublished             = {http://bit.ly/cern-lhc},
  Year                     = {2015},

  Institution              = {CERN},
  Url                      = {http://bit.ly/cern-lhc}
}

@Misc{CERNWLCG,
  Title                    = {{The Worldwide LHC Computing Grid}},

  Author                   = {CERN},
  HowPublished             = {http://bit.ly/cern-wlcg},
  Year                     = {2015},

  Institution              = {CERN},
  Url                      = {http://bit.ly/cern-wlcg}
}

@Article{Chen2014,
  Title                    = {{Big data: A survey}},
  Author                   = {Chen, Min and Mao, Shiwen and Liu, Yunhao},
  Journal                  = {Mobile Networks and Applications},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {171--209},
  Volume                   = {19},

  Abstract                 = {In this paper, we review the background and state-of-the-art of big data. We first introduce the general background of big data and review related technologies, such as could computing, Internet of Things, data centers, and Hadoop.We then focus on the four phases of the value chain of big data, i.e., data generation, data acquisition, data storage, and data analysis. For each phase, we introduce the general background, discuss the technical challenges, and review the latest advances. We finally examine the several representative applications of big data, including enterprise management, Internet of Things, online social networks, medial applications, collective intelligence, and smart grid. These discussions aimto provide a comprehensive overview and big-picture to readers of this exciting area. This survey is concluded with a discussion of open problems and future directions.},
  Doi                      = {10.1007/s11036-013-0489-0},
  File                     = {:home/giacomo/Documents/research/mendeley library/Big data A survey. 2014. Chen, Mao, Liu.pdf:pdf},
  ISBN                     = {1383-469X},
  ISSN                     = {1383469X},
  Keywords                 = {Big data,Big data analysis,Cloud computing,Data center,Hadoop,Internet of things,Smart grid}
}

@Book{Christensen2013,
  Title                    = {{The Innovator's Dilemma: When New Technologies Cause Great Firms to Fail}},
  Author                   = {Christensen, Clayton M.},
  Publisher                = {Harvard Business Review Press},
  Year                     = {2013},

  ISBN                     = {9781422196021}
}

@TechReport{Cisco2015:VNI,
  Title                    = {{Cisco Visual Networking Index: Forecast and Methodology, 2014 - 2019}},
  Author                   = {Cisco},
  Institution              = {Cisco},
  Year                     = {2015},

  Abstract                 = {This forecast is part of the Cisco Visual Networking Index™ (Cisco VNI™), an ongoing initiative to track and forecast the impact of visual networking applications. This document presents the details of the Cisco VNI global IP traffic forecast and the methodology behind it.},
  File                     = {:home/giacomo/Documents/research/mendeley library/VNI Forecast and Methodology, 2014 – 2019. 2015. Cisco.pdf:pdf},
  Pages                    = {14}
}

@TechReport{Cisco2015:VNI:Mobile,
  Title                    = {{VNI Global Mobile Data Traffic Forecast Update, 2014-2019}},
  Author                   = {Cisco},
  Institution              = {Cisco},
  Year                     = {2015},

  Booktitle                = {Cisco Visual Networking Index},
  File                     = {:home/giacomo/Documents/research/mendeley library/VNI Global Mobile Data Traffic Forecast Update, 2014 – 2019. 2015. Cisco.pdf:pdf},
  Pages                    = {1--42}
}

@TechReport{Cisco2015:Zetta,
  Title                    = {{The Zettabyte Era: Trends and Analysis}},
  Author                   = {Cisco},
  Institution              = {Cisco},
  Year                     = {2015},
  Number                   = {May 2015},

  Abstract                 = {This document is part of the Cisco® Visual Networking Index (VNI), an ongoing initiative to track and forecast the impact of visual networking applications. The document presents some of the main findings of Cisco’s global IP traffic forecast and explores the implications of IP traffic growth for service providers.},
  Booktitle                = {Cisco Visual Networking Index},
  File                     = {:home/giacomo/Documents/research/mendeley library/The Zettabyte Era Trends and Analysis. 2015. Cisco.pdf:pdf},
  Pages                    = {29},
  Url                      = {http://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/VNI{\_}Hyperconnectivity{\_}WP.html}
}

@Article{Cugola2012,
  Title                    = {{Processing Flows of Information: From Data Stream to Complex Event Processing}},
  Author                   = {Cugola, Gianpaolo and Margara, Alessandro},
  Journal                  = {ACM Comput. Surv.},
  Year                     = {2012},
  Number                   = {i},
  Pages                    = {15:1--15:62},
  Volume                   = {44},

  Abstract                 = {A large number of distributed applications requires continuous and timely processing of information as it flows from the periphery to the center of the system. Examples include intrusion detection systems which analyze network traffic in real-time to identify possible attacks; environmental monitoring applications which process raw data coming from sensor networks to identify critical situations; or applications performing online analysis of stock prices to identify trends and forecast future values.$\backslash$n$\backslash$nTraditional DBMSs, which need to store and index data before processing it, can hardly fulfill the requirements of timeliness coming from such domains. Accordingly, during the last decade, different research communities developed a number of tools, which we collectively call Information flow processing (IFP) systems, to support these scenarios. They differ in their system architecture, data model, rule model, and rule language. In this article, we survey these systems to help researchers, who often come from different backgrounds, in understanding how the various approaches they adopt may complement each other.$\backslash$n$\backslash$nIn particular, we propose a general, unifying model to capture the different aspects of an IFP system and use it to provide a complete and precise classification of the systems and mechanisms proposed so far.},
  Doi                      = {10.1145/2187671.2187677},
  File                     = {:home/giacomo/Documents/research/mendeley library/Processing Flows of Information From Data Stream to Complex Event Processing. 2012. Cugola, Margara.pdf:pdf},
  ISBN                     = {0360-0300},
  ISSN                     = {0360-0300},
  Url                      = {http://doi.acm.org/10.1145/2187671.2187677}
}

@Article{Curry2014,
  Title                    = {{Computational Complexity Measures for Many-objective Optimization Problems}},
  Author                   = {Curry, David M. and Dagli, Cihan H.},
  Journal                  = {Procedia Computer Science},
  Year                     = {2014},
  Pages                    = {185--191},
  Volume                   = {36},

  Abstract                 = {MULTI-OBJECTIVE
Optimization Problems (MOPs) are commonly encountered in the study and design of complex systems. Pareto dominance is the most common relationship used to compare solutions in MOPs, however as the number of objectives grows beyond three, Pareto dominance alone is no longer satisfactory. These problems are termed “Many-Objective Optimization Problems (MaOPs)”. While most MaOP algorithms are modifications of common MOP algorithms, determining the impact on their computational complexity is difficult. This paper defines computational complexity measures for these algorithms and applies these measures to a Multi-Objective Evolutionary Algorithm (MOEA) and its MaOP counterpart.},
  Doi                      = {10.1016/j.procs.2014.09.077},
  File                     = {:home/giacomo/Documents/research/mendeley library/Computational Complexity Measures for Many-objective Optimization Problems. 2014. Curry, Dagli.pdf:pdf},
  ISSN                     = {18770509},
  Keywords                 = {MOEA,MOP,MaOP,computational complexity,many-objective optimization problem,multi- objective evolutionary algorithm,multi-objective optimization problem},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S187705091401326X}
}

@Article{Dantzig1955,
  Title                    = {{The Generalized Simplex Method}},
  Author                   = {Dantzig, George Bernard and Ordern, Alexander and Wolfe, Philip},
  Journal                  = {Pacific Journal of Mathematics},
  Year                     = {1955},
  Number                   = {2},
  Pages                    = {183--195},
  Volume                   = {5},

  Doi                      = {10.2140/pjm.1955.5-2},
  File                     = {:home/giacomo/Documents/research/mendeley library/The Generalized Simplex Method. 1955. Dantzig, Ordern, Wolfe.pdf:pdf},
  ISSN                     = {0030-8730},
  Keywords                 = {operation research,simplex},
  Mendeley-tags            = {operation research,simplex}
}

@Article{Eugster2003,
  Title                    = {{The many faces of publish/subscribe}},
  Author                   = {Eugster, Patrick TH. and Felber, Pascal A. and Guerraoui, Rachid and Kermarrec, Anne-Marie},
  Journal                  = {ACM Computing Surveys},
  Year                     = {2003},
  Number                   = {2},
  Pages                    = {114--131},
  Volume                   = {35},

  Abstract                 = {Well adapted to the loosely coupled nature of distributed interaction in large-scale applications, the publish/subscribe communication paradigm has recently received increasing attention.With systems based on the publish/subscribe interaction scheme, subscribers register their interest in an event, or a pattern of events, and are subsequently asynchronously notified of events generated by publishers. Many variants of the paradigm have recently been proposed, each variant being specifically adapted to some given application or network model. This paper factors out the common denominator underlying these variants: full decoupling of the communicating entities in time, space, and synchronization.We use these three decoupling dimensions to better identify commonalities and divergences with traditional interaction paradigms. The many variations on the theme of publish/subscribe are classified and synthesized. In particular, their respective benefits and shortcomings are discussed both in terms of interfaces and implementations.},
  Doi                      = {10.1145/857076.857078},
  File                     = {:home/giacomo/Documents/research/mendeley library/The many faces of publishsubscribe. 2003. Eugster et al.pdf:pdf},
  ISBN                     = {0360-0300},
  ISSN                     = {03600300},
  Pmid                     = {22171980},
  Url                      = {http://portal.acm.org/citation.cfm?doid=857076.857078}
}

@Article{Fortnow2002,
  Title                    = {{A Short History of Computational Complexity}},
  Author                   = {Fortnow, Lance and Homer, Steve},
  Journal                  = {Science},
  Year                     = {2002},
  Pages                    = {1--26},

  File                     = {:home/giacomo/Documents/research/mendeley library/A Short History of Computational Complexity. 2002. Fortnow, Homer.pdf:pdf}
}

@Article{Gandomi2015,
  Title                    = {{Beyond the hype: Big data concepts, methods, and analytics}},
  Author                   = {Gandomi, Amir and Haider, Murtaza},
  Journal                  = {International Journal of Information Management},
  Year                     = {2015},

  Month                    = {apr},
  Number                   = {2},
  Pages                    = {137--144},
  Volume                   = {35},

  Abstract                 = {Size is the first, and at times, the only dimension that leaps out at the mention of big data. This paper attempts to offer a broader definition of big data that captures its other unique and defining characteristics. The rapid evolution and adoption of big data by industry has leapfrogged the discourse to popular outlets, forcing the academic press to catch up. Academic journals in numerous disciplines, which will benefit from a relevant discussion of big data, have yet to cover the topic. This paper presents a consolidated description of big data by integrating definitions from practitioners and academics. The paper's primary focus is on the analytic methods used for big data. A particular distinguishing feature of this paper is its focus on analytics related to unstructured data, which constitute 95{\%} of big data. This paper highlights the need to develop appropriate and efficient analytical methods to leverage massive volumes of heterogeneous data in unstructured text, audio, and video formats. This paper also reinforces the need to devise new tools for predictive analytics for structured big data. The statistical methods in practice were devised to infer from sample data. The heterogeneity, noise, and the massive size of structured big data calls for developing computationally efficient algorithms that may avoid big data pitfalls, such as spurious correlation.},
  Doi                      = {10.1016/j.ijinfomgt.2014.10.007},
  File                     = {:home/giacomo/Documents/research/mendeley library/Beyond the hype Big data concepts, methods, and analytics. 2015. Gandomi, Haider.pdf:pdf},
  ISSN                     = {02684012},
  Keywords                 = {5Vs,Big data analytics,Big data definition,Predictive analytics,Unstructured data analytics,big data,distributed systems},
  Mendeley-tags            = {5Vs,big data,distributed systems},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0268401214001066}
}

@TechReport{Gantz2007,
  Title                    = {{The Expanding Digital Universe}},
  Author                   = {Gantz, John},
  Institution              = {IDC},
  Year                     = {2007},

  Abstract                 = {The airwaves, telephone circuits, and computer cables are buzzing. Digital information surrounds us. We see digital bits on our new HDTVs, listen to them over the Internet, and create new ones ourselves every time we take a picture with our digital cameras. Then we email them to friends and family and create more digital bits. There's no secret here. YouTube, a company that didnt exist just a few years ago, hosts 100 million video streams a day.i Experts say more than a billion songs a day are shared over the Internet in MP3 format.ii Digital bits. London's 200 traffic surveillance cameras send 64 trillion bits a day to the command data center.iiiChevron's CIO says his company accumulates data at the rate of 2 terabytes 17,592,000,000,000 bits a day.iv TV broadcasting is going all-digital by the end of the decade in most countries. More digital bits. What is a secret one staring us in the face is how much all these bits add up to, how fast they are multiplying, and what their proliferation imply. This White Paper, sponsored by EMC, is IDC's forecast of the digital universe all the 1s and 0s created, captured, and replicated and the implications for those who take the photos, share the music, and generate the digital bits and those who organize, secure, and manage the access to and storage of the information.},
  Booktitle                = {IDC White Paper},
  Doi                      = {10.1002/humu.21252},
  File                     = {:home/giacomo/Documents/research/mendeley library/The Expanding Digital Universe. 2007. Gantz.pdf:pdf},
  ISSN                     = {10981004},
  Pages                    = {24},
  Pmid                     = {20513141}
}

@Article{Gantz2012,
  Title                    = {{The Digital Universe in 2020}},
  Author                   = {Gantz, John and Reinsel, David and Shadows, Bigger Digital},
  Journal                  = {IDC iView "Big Data, Bigger Digital Shadows, and Biggest Growth in the Far East"},
  Year                     = {2012},
  Number                   = {December 2012},
  Pages                    = {1--16},
  Volume                   = {2007},

  Abstract                 = {Welcome to the "digital universe" — a measure of all the digital data created, replicated, and consumed in a single year. It's also a projection of the size of that universe to the end of the decade.},
  File                     = {:home/giacomo/Documents/research/mendeley library/The Digital Universe in 2020. 2012. Gantz, Reinsel, Shadows.pdf:pdf}
}

@Misc{GoogleTrendsHotSearches,
  Title                    = {{Hot Searches Trends}},

  Author                   = {Google},
  HowPublished             = {http://bit.ly/google-hot-searches},
  Year                     = {2015},

  Booktitle                = {Google Trends},
  Institution              = {Google},
  Url                      = {http://bit.ly/google-hot-searches}
}

@Article{Hashem2014,
  Title                    = {{The rise of “big data” on cloud computing: Review and open research issues}},
  Author                   = {Hashem, Ibrahim Abaker Targio and Yaqoob, Ibrar and Anuar, Nor Badrul and Mokhtar, Salimah and Gani, Abdullah and {Ullah Khan}, Samee},
  Journal                  = {Information Systems},
  Year                     = {2015},

  Month                    = {jan},
  Pages                    = {98--115},
  Volume                   = {47},

  Abstract                 = {Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized.},
  Doi                      = {10.1016/j.is.2014.07.006},
  File                     = {:home/giacomo/Documents/research/mendeley library/The rise of “big data” on cloud computing Review and open research issues. 2015. Hashem et al.pdf:pdf},
  ISBN                     = {0306-4379},
  ISSN                     = {03064379},
  Keywords                 = {Big data,Cloud computing,Hadoop},
  Publisher                = {Elsevier},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0306437914001288}
}

@InProceedings{Heinze2013,
  Title                    = {{Elastic Complex Event Processing under Varying Query Load}},
  Author                   = {Heinze, Thomas and Ji, Yuanzhen and Pan, Yinying and Gr{\"{u}}neberger, Franz Josef and Jerzak, Zbigniew and Fetzer, Christof},
  Booktitle                = {First International Workshop on Big Dynamic Distributed Data},
  Year                     = {2013},
  Number                   = {1},
  Organization             = {Citeseer},
  Pages                    = {1--6},

  Abstract                 = {Distributed data stream processing systems, like Twitter Storm or Yahoo! S4, have been primarily focusing on adapting to varying event rates. However, as these systems are becoming increasingly multi-tenant, adaptation to the varying query load is becoming an equally important problem. In this paper we present FUGU – an elastic allocator for Complex Event Processing systems. FUGU uses bin packing to allocate continuous queries to a varying set of nodes. Driven by elasticity requirements FUGU maximizes the overall system utilization while trying to maintain stable processing latencies. The specific contributions of this paper are: (1) introduc- tion of a re-balancing scheme for bin packing allowing FUGU to increase overall system utilization by six percent and (2) a detailed study of achievable system utilization and latency under real-life workload from Frankfurt Stock Exchange.},
  File                     = {:home/giacomo/Documents/research/mendeley library/Elastic Complex Event Processing under Varying Query Load. 2013. Heinze et al.pdf:pdf},
  ISSN                     = {16130073},
  Url                      = {http://db.disi.unitn.eu/pages/VLDBProgram/pdf/BD3/paper4.pdf}
}

@Misc{IBMCPLEXCPOpt,
  Title                    = {{IBM CPLEX CP Optimizer}},

  Author                   = {IBM},
  HowPublished             = {http://bit.ly/ibm-cplex-cp-optimizer},
  Year                     = {2015},

  Institution              = {IBM},
  Keywords                 = {cplex,ibm},
  Mendeley-tags            = {cplex,ibm},
  Url                      = {http://bit.ly/ibm-cplex-cp-optimizer}
}

@Misc{IBMCPLEXOpt,
  Title                    = {{IBM CPLEX Optimizer}},

  Author                   = {IBM},
  HowPublished             = {http://bit.ly/ibm-cplex-optimizer},
  Year                     = {2015},

  Institution              = {IBM},
  Keywords                 = {cplex,ibm},
  Mendeley-tags            = {cplex,ibm},
  Url                      = {http://bit.ly/ibm-cplex-optimizer}
}

@Misc{IBMCPLEXOptPerformance,
  Title                    = {{IBM CPLEX Optimizers Performance}},

  Author                   = {IBM},
  HowPublished             = {http://bit.ly/ibm-cplex-opt-perfomance},
  Year                     = {2015},

  Institution              = {IBM},
  Keywords                 = {cplex,ibm},
  Mendeley-tags            = {cplex,ibm},
  Url                      = {http://bit.ly/ibm-cplex-opt-perfomance}
}

@Misc{IBMCPLEXOptStudioOverview,
  Title                    = {{IBM ILOG CPLEX Optimization Studio}},

  Author                   = {IBM},
  HowPublished             = {http://bit.ly/ibm-ilog-cplex-opt-studio},
  Year                     = {2015},

  Institution              = {IBM},
  Keywords                 = {cplex,ibm},
  Mendeley-tags            = {cplex,ibm},
  Url                      = {http://bit.ly/ibm-ilog-cplex-opt-studio}
}

@Misc{IBMILOGOverview,
  Title                    = {{ILOG is now part of IBM}},

  Author                   = {IBM},
  HowPublished             = {http://bit.ly/ibm-ilog-overview},
  Year                     = {2015},

  Institution              = {IBM},
  Keywords                 = {cplex,ibm},
  Mendeley-tags            = {cplex,ibm},
  Url                      = {http://bit.ly/ibm-ilog-overview}
}

@Misc{IBMMPvsCP,
  Title                    = {{Mathematical Programming vs. Constraint Programming}},

  Author                   = {IBM},
  HowPublished             = {http://bit.ly/ibm-mp-vs-cp},
  Year                     = {2015},

  Institution              = {IBM},
  Keywords                 = {cplex,ibm},
  Mendeley-tags            = {cplex,ibm},
  Url                      = {http://bit.ly/ibm-mp-vs-cp}
}

@Manual{IBMCPLEXManualIDE,
  Title                    = {{Getting Started with the CPLEX Studio IDE}},

  Address                  = {New York, NY, USA},
  Author                   = {IBM},
  Organization             = {IBM},
  Year                     = {2014},

  File                     = {:home/giacomo/Documents/research/mendeley library/Getting Started with the CPLEX Studio IDE. 2014. IBM.pdf:pdf},
  Institution              = {IBM},
  Keywords                 = {cplex,ibm},
  Mendeley-tags            = {cplex,ibm},
  Pages                    = {96}
}

@Manual{IBMCPLEXUserManual,
  Title                    = {{CPLEX User's Manual}},

  Address                  = {New York, NY, USA},
  Author                   = {IBM},
  Year                     = {2014},

  File                     = {:home/giacomo/Documents/research/mendeley library//CPLEX User's Manual. 2014. IBM.pdf:pdf},
  Institution              = {IBM},
  Keywords                 = {cplex,ibm},
  Mendeley-tags            = {cplex,ibm},
  Pages                    = {552}
}

@Manual{IBMOPLReferenceManual,
  Title                    = {{OPL Language Reference Manual}},

  Address                  = {New York, NY, USA},
  Author                   = {IBM},
  Year                     = {2014},

  File                     = {:home/giacomo/Documents/research/mendeley library//OPL Language Reference Manual. 2014. IBM.pdf:pdf},
  Institution              = {IBM},
  Keywords                 = {cplex,ibm},
  Mendeley-tags            = {cplex,ibm},
  Pages                    = {150}
}

@Manual{IBMOPLUserManual,
  Title                    = {{OPL Language User's Manual}},

  Address                  = {New York, NY, USA},
  Author                   = {IBM},
  Year                     = {2014},

  File                     = {:home/giacomo/Documents/research/mendeley library//OPL Language User's Manual. 2014. IBM.pdf:pdf},
  Institution              = {IBM},
  Keywords                 = {cplex,ibm},
  Mendeley-tags            = {cplex,ibm},
  Pages                    = {168}
}

@Misc{IBMILOGAcquisition,
  Title                    = {{IBM's ILOG acquisition}},

  Author                   = {IBM},
  HowPublished             = {http://bit.ly/ibm-ilog-acquisition},
  Year                     = {2009},

  Institution              = {IBM Press},
  Keywords                 = {cplex,ibm},
  Mendeley-tags            = {cplex,ibm},
  Url                      = {http://bit.ly/ibm-ilog-acquisition}
}

@Misc{JCMITDiskPrice,
  Title                    = {{Disk Drive Prices (1955-2015)}},

  Author                   = {JCMIT},
  HowPublished             = {http://bit.ly/jcmit-disk-price-1955-2015},
  Year                     = {2015},

  Institution              = {JCMIT},
  Url                      = {http://bit.ly/jcmit-disk-price-1955-2015}
}

@InProceedings{Jerzak2012,
  Title                    = {{The DEBS 2012 grand challenge}},
  Author                   = {Jerzak, Zbigniew and Heinze, Thomas and Fehr, Matthias and Gr{\"{o}}ber, Daniel and Hartung, Raik and Stojanovic, Nenad},
  Booktitle                = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems - DEBS '12},
  Year                     = {2012},

  Address                  = {New York, New York, USA},
  Organization             = {ACM},
  Pages                    = {393--398},
  Publisher                = {ACM Press},

  Abstract                 = {The goal of the DEBS Grand Challenge series is to contribute to the Event Processing Grand Challenge, that serves as a common goal and mechanism for coordinating research focusing on event processing. DEBS Grand Challenge series provides a common ground and evaluation criteria for a com- petition aimed at both research and industrial event-based systems. The goal of the DEBS Grand Challenge participants is to implement a solution to a specific problem provided by the DEBS Grand Challenge organizers. In this paper we present a description of the DEBS 2012 Grand Challenge problem focusing on the high-tech manufacturing domain. Moreover we provide a set of both: (1) real-life data and (2) queries which can be used by the DEBS 2012 Grand Challenge participants as well as research community at},
  Annote                   = {problem domain: hi-tech manufacturing monitoring
queries: energy consumption, relation between sensors states},
  Doi                      = {10.1145/2335484.2335536},
  File                     = {:home/giacomo/Documents/research/mendeley library/The DEBS 2012 grand challenge. 2012. Jerzak et al.pdf:pdf},
  ISBN                     = {9781450313155},
  Keywords                 = {cep,data stream processing,debs,event processing,streaming},
  Mendeley-tags            = {data stream processing,debs},
  Url                      = {http://dl.acm.org/citation.cfm?doid=2335484.2335536$\backslash$nhttp://dl.acm.org/citation.cfm?id=2335484.2335536 http://dl.acm.org/citation.cfm?doid=2335484.2335536}
}

@InProceedings{JerzakZiekow2015,
  Title                    = {{The DEBS 2015 grand challenge}},
  Author                   = {Jerzak, Zbigniew and Ziekow, Holger},
  Booktitle                = {Proceedings of the 9th ACM International Conference on Distributed Event-Based Systems - DEBS '15},
  Year                     = {2015},

  Address                  = {New York, New York, USA},
  Organization             = {ACM},
  Pages                    = {266--268},
  Publisher                = {ACM Press},

  Abstract                 = {The focus of the DEBS 2015 Grand Challenge is on processing of data streams originating from the New York City Taxi and Limousine Commission. The data is made available under the Freedomof Information Law and provides information pickup, drop off, and payments made in New York City medallion taxis. The goal of the DEBS 2015 Grand Challenge is to process the spatio-temporal data streams and calculate real- time indicators of most frequent routes and most profitable areas in the New York City.},
  Annote                   = {problem domain: taxy management
queries: frequent routes, profitable areas},
  Doi                      = {10.1145/2675743.2772598},
  File                     = {:home/giacomo/Documents/research/mendeley library/The DEBS 2015 grand challenge. 2015. Jerzak, Ziekow.pdf:pdf},
  ISBN                     = {9781450332866},
  Keywords                 = {data stream processing,debs,event processing,geo-spatial,streaming,utilities},
  Mendeley-tags            = {data stream processing,debs},
  Url                      = {http://doi.acm.org/10.1145/2675743.2772598 http://dl.acm.org/citation.cfm?doid=2675743.2772598}
}

@InProceedings{JerzakZiekow2014,
  Title                    = {{The DEBS 2014 grand challenge}},
  Author                   = {Jerzak, Zbigniew and Ziekow, Holger},
  Booktitle                = {Proceedings of the 8th ACM International Conference on Distributed Event-Based Systems - DEBS '14},
  Year                     = {2014},

  Address                  = {New York, New York, USA},
  Organization             = {ACM},
  Pages                    = {266--269},
  Publisher                = {ACM Press},

  Abstract                 = {Event processing systems in general and data stream pro- cessing systems in particular focus on processing of queries over unbounded event streams. The goal of the DEBS 2014 Grand Challenge is to provide a specific problem, originating from the domain of energy data management, which can be leveraged by both commercial and academic event processing systems. The problem provided by this year’s challenge seeks to highlight the capability of event stream processing systems towards combining of the processing of live and historical event data as well as scaling out for coping with high velocity data streams and complex analysis.},
  Annote                   = {problem domain: energy data management
queries: short-term load forecasting, outliers detection for real-time demand management},
  Doi                      = {10.1145/2611286.2611333},
  File                     = {:home/giacomo/Documents/research/mendeley library/The DEBS 2014 grand challenge. 2014. Jerzak, Ziekow.pdf:pdf},
  ISBN                     = {9781450327374},
  Keywords                 = {data stream processing,debs,event processing,streaming,utilities},
  Mendeley-tags            = {data stream processing,debs},
  Url                      = {http://doi.acm.org/10.1145/2611286.2611333 http://dl.acm.org/citation.cfm?doid=2611286.2611333}
}

@Misc{JUnitHome,
  Title                    = {{JUnit}},

  Author                   = {JUnit},
  HowPublished             = {http://bit.ly/junit-home},
  Year                     = {2015},

  Institution              = {JUnit},
  Keywords                 = {java,junit,test},
  Mendeley-tags            = {java,junit,test},
  Url                      = {http://bit.ly/junit-home}
}

@Article{Kaisler2013,
  Title                    = {{Big Data: Issues and Challenges Moving Forward}},
  Author                   = {Kaisler, S and Armour, F and Espinosa, J a and Money, W},
  Journal                  = {46th Hawaii International Conference on System Sciences (HICSS)},
  Year                     = {2013},
  Pages                    = {995--1004},

  Abstract                 = {Big data refers to data volumes in the range of exabytes (1018) and beyond. Such volumes exceed the capacity of current on-line storage systems and processing systems. Data, information, and knowledge are being created and collected at a rate that is rapidly approaching the exabyte/year range. But, its creation and aggregation are accelerating and will approach the zettabyte/year range within a few years. Volume is only one aspect of big data; other attributes are variety, velocity, value, and complexity. Storage and data transport are technology issues, which seem to be solvable in the near-term, but represent longterm challenges that require research and new paradigms. We analyze the issues and challenges as we begin a collaborative research program into methodologies for big data analysis and design.},
  Doi                      = {10.1109/HICSS.2013.645},
  File                     = {:home/giacomo/Documents/research/mendeley library/Big Data Issues and Challenges Moving Forward. 2013. Kaisler et al.pdf:pdf},
  ISBN                     = {978-1-4673-5933-7},
  ISSN                     = {1530-1605},
  Keywords                 = {Data handling,Data storage systems,Distributed databases,Information management,Media,Organizations,big data,collaborative research program,data analysis,data transport,data volumes,online storage systems},
  Url                      = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6479953}
}

@Book{Kellerer2004,
  Title                    = {{Knapsack Problems}},
  Author                   = {Kellerer, Hans and Pferschy, Ulrich and Pisinger, David},
  Publisher                = {Springer-Verlag},
  Year                     = {2004},
  Number                   = {2},
  Volume                   = {33},

  Abstract                 = {This book provides a full-scale presentation of all methods and techniques available for the solution of the Knapsack problem. This most basic combinatorial optimization problem appears explicitly or as a subproblem in a wide range of optimization models with backgrounds such diverse as cutting and packing, finance, logistics or general integer programming. This monograph spans the range from a comprehensive introduction of classical algorithmic methods to the unified presentation of the most recent and advanced results in this area many of them originating from the authors. The chapters dealing with particular versions and extensions of the Knapsack problem are self-contained to a high degree and provide a valuable source of reference for researchers. Due to its simple structure, the Knapsack problem is an ideal model for introducing solution techniques to students of computer science, mathematics and economics. The first three chapters give an in-depth treatment of several basic techniques, making the book also suitable as underlying literature for courses in combinatorial optimization and approximation.},
  Booktitle                = {Operations Research Letters},
  Doi                      = {10.1007/978-3-540-24777-7},
  ISBN                     = {978-3-642-07311-3},
  ISSN                     = {01676377},
  Pages                    = {546}
}

@Article{Kelly2015,
  Title                    = {{Big Data Vendor Revenue And Market Forecast 2012-2017}},
  Author                   = {Kelly, Jeff and Floyer, David and Vellante, Dave and Miniman, Stu},
  Journal                  = {Wikibon},
  Year                     = {2015},
  Number                   = {March},
  Pages                    = {1},

  Abstract                 = {he hype surrounding Big Data, which showed no signs of abating in 2012, now has big dollars backing it up. Factory revenue generated by the sale of Big Data-related hardware, software and services took a major step forward in 2012, growing by 59{\%} over 2011(a).$\backslash$n$\backslash$nThe total Big Data market reached {\$}11.59 billion in 2012, ahead of Wikibon’s 2011 forecast. The Big Data market is projected to reach {\$}18.1 billion in 2013, an annual growth of 61{\%}. This puts it on pace to exceed {\$}47 billion by 2017. That translates to a 31{\%} compound annual growth rate over the five year period 2012-2017.},
  File                     = {:home/giacomo/Documents/research/mendeley library/Big Data Vendor Revenue And Market Forecast 2012-2017. 2015. Kelly et al.pdf:pdf},
  Url                      = {http://wikibon.org/wiki/v/Big{\_}Data{\_}Vendor{\_}Revenue{\_}and{\_}Market{\_}Forecast{\_}2012-2017}
}

@Misc{InternetPulse,
  Title                    = {{Internet Pulse}},

  Author                   = {Keynote},
  HowPublished             = {http://bit.ly/internet-pulse},
  Year                     = {2015},

  Institution              = {Keynote},
  Url                      = {http://bit.ly/internet-pulse},
  Urldate                  = {2015-10-20}
}

@Article{Krentel1988,
  Title                    = {{The complexity of optimization problems}},
  Author                   = {Krentel, Mark W.},
  Journal                  = {Journal of Computer and System Sciences},
  Year                     = {1988},

  Month                    = {jun},
  Number                   = {3},
  Pages                    = {490--509},
  Volume                   = {36},

  Abstract                 = {We consider NP-complete optimization problems at the level of computing their optimal value, and define a class of functions called OptP to capture this level of structure. We show that TRAVELING SALESPERSON and KNAPSACK are complete for OptP, and that CLIQUE and COLORING are complete for a subclass of OptP. These results show a deeper level of structure in these problems than was previously known. We also show that OptP is closely related to FPSAT, the class of functions computable in polynomial time with an oracle for NP. This allows us to quantify exactly “how much” NP-completeness is in these problems. In particular, in this measure, we show that TRAVELING SALESPERSON is strictly harder than CLIQUE and that CLIQUE is strictly harder than BIN PACKING. A further result is that an OptP-completeness result implies NP-, DP-, and {\&}completeness results, thus tying these four classes Closely together.},
  Doi                      = {10.1016/0022-0000(88)90039-6},
  File                     = {:home/giacomo/Documents/research/mendeley library/The complexity of optimization problems. 1988. Krentel.pdf:pdf},
  ISBN                     = {0897911938},
  ISSN                     = {00220000},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/0022000088900396}
}

@Unpublished{LoPresti2015b,
  Title                    = {{Optimal DSP Deployment}},
  Author                   = {{Lo Presti}, Francesco},
  Year                     = {2015},

  Address                  = {Rome, Italy},
  File                     = {:home/giacomo/Documents/research/mendeley library/Optimal DSP Deployment. 2015. Lo Presti(2).pdf:pdf},
  Institution              = {University of Rome Tor Vergata},
  Pages                    = {1--7}
}

@Article{Mahajan2002,
  Title                    = {{Inferring link weights using end-to-end measurements}},
  Author                   = {Mahajan, Ratul and Spring, Neil and Wetherall, David and Anderson, Tom},
  Journal                  = {Proceedings of the second ACM SIGCOMM Workshop on Internet measurment workshop - IMW '02},
  Year                     = {2002},
  Pages                    = {231},

  Abstract                 = {We describe a novel constraint-based approach to approximate ISP link weights using only end-to-end measurements. Common routing protocols such as OSPF and IS-IS choose least-cost paths using link weights, so inferred weights provide a simple, concise, and useful model of intradomain routing. Our approach extends router-level ISP maps, which include only connectivity, with link weights that are consistent with routing. Our inferred weights agree well with observed routing: while our inferred weights fully characterize the set of shortest paths between 84-99{\%} of the router-pairs, alternative models based on hop count and latency do so for only 47-81{\%} of the pairs.},
  Doi                      = {10.1145/637235.637237},
  File                     = {:home/giacomo/Documents/research/mendeley library/Inferring link weights using end-to-end measurements. 2002. Mahajan et al.pdf:pdf},
  ISBN                     = {158113603X},
  Url                      = {http://portal.acm.org/citation.cfm?doid=637201.637237}
}

@Misc{OPMapGit,
  Title                    = {{OPMap on Github}},

  Author                   = {Marciani, Giacomo},
  HowPublished             = {http://bit.ly/opmap-github},
  Year                     = {2015},

  Institution              = {University of Rome Tor Vergata},
  Keywords                 = {github,opmap},
  Mendeley-tags            = {github,opmap},
  Url                      = {http://bit.ly/opmap-github}
}

@Misc{OPMapOPLGit,
  Title                    = {{OPMap-OPL on Github}},

  Author                   = {Marciani, Giacomo},
  HowPublished             = {http://bit.ly/opmap-opl-github},
  Year                     = {2015},

  Institution              = {University of Rome Tor Vergata},
  Keywords                 = {github,opmap},
  Mendeley-tags            = {github,opmap},
  Url                      = {http://bit.ly/opmap-opl-github}
}

@Book{Marr2015,
  Title                    = {{Big Data Case Study Collection}},
  Author                   = {Marr, Bernard},
  Publisher                = {Wiley},
  Year                     = {2015},
  Edition                  = {1},

  Abstract                 = {Big Data is a big thing and this case study collection will give you a good overview of how some companies really leverage big data to drive business performance. They range from industry giants like Google, Amazon, Facebook, GE, and Microsoft, to smaller businesses which have put big data at the centre of their business model, like Kaggle and Cornerstone. This case study collection is based on articles published by Bernard Marr on his LinkedIn Influencer blog.},
  File                     = {:home/giacomo/Documents/research/mendeley library/Big Data Case Study Collection. 2015. Marr.pdf:pdf},
  Pages                    = {32}
}

@Book{Martello1987,
  Title                    = {{Surveys in Combinatorial Optimization}},
  Author                   = {Martello, Silvano and Laporte, Gilbert and Minoux, Michel and Ribiero, Celso},
  Editor                   = {Hammer, Peter},
  Publisher                = {Elsevier},
  Year                     = {1987},
  Volume                   = {31},

  Booktitle                = {Surveys in Combinatorial Optimization},
  Doi                      = {10.1016/S0304-0208(08)73235-3},
  File                     = {:home/giacomo/Documents/research/mendeley library/Surveys in Combinatorial Optimization. 1987. Martello et al.pdf:pdf},
  ISBN                     = {9780444701367},
  ISSN                     = {03040208},
  Pages                    = {147--184}
}

@Book{Martello1990,
  Title                    = {{Knapsack Problems: Algorithms and Computer Implementations}},
  Author                   = {Martello, Silvano and Toth, Paolo},
  Editor                   = {Wiley},
  Publisher                = {Wiley},
  Year                     = {1990},
  Edition                  = {1},

  File                     = {:home/giacomo/Documents/research/mendeley library/Knapsack Problems Algorithms and Computer Implementations. 1990. Martello, Toth.pdf:pdf},
  ISBN                     = {0-471-92420-2},
  Pages                    = {1--306}
}

@Article{Martello1987a,
  Title                    = {{Algorithms for Knapsack Problems}},
  Author                   = {Martello, Silvano and Toth, Paolo},
  Journal                  = {North-Holland Mathematics Studies},
  Year                     = {1987},
  Number                   = {C},
  Pages                    = {213--257},
  Volume                   = {132},

  Abstract                 = {This thesis considers a family of combinatorial problems known under the name Knapsack Problems. As all the problems are A7)-hard we are searching for exact solution techniques having reasonable solution times for nearly all instances encountered in practice, despite having exponential time bounds for a number of highly contrived problem instances. A similar behavior is known from the Simplex algorithm, which despite its exponential worst-case behavior has reasonable solution times for all realistic problems.}
}

@TechReport{MITPathPersuasion2015,
  Title                    = {{Path of Persuasion}},
  Author                   = {MIT, Technology Review},
  Institution              = {MIT},
  Year                     = {2015},

  Address                  = {Cambridge, MA, USA},

  Abstract                 = {How technologies from smartphones to social media are used to influence our tastes, behavior, and even habits.},
  File                     = {:home/giacomo/Documents/research/mendeley library/Path of Persuasion. 2015. MIT.pdf:pdf},
  Keywords                 = {big data,persuasive technologies},
  Mendeley-tags            = {big data,persuasive technologies},
  Pages                    = {15}
}

@InProceedings{Mutschler2013,
  Title                    = {{The DEBS 2013 grand challenge}},
  Author                   = {Mutschler, Christopher and Ziekow, Holger and Jerzak, Zbigniew},
  Booktitle                = {Proceedings of the 7th ACM international conference on Distributed event-based systems - DEBS '13},
  Year                     = {2013},

  Address                  = {New York, New York, USA},
  Organization             = {ACM},
  Pages                    = {289},
  Publisher                = {ACM Press},

  Abstract                 = {The ACM DEBS 2013 Grand Challenge is the third in a series of challenges which seek to provide a common ground and evaluation criteria for a competition aimed at both research and industrial event-based systems. The goal of the Grand Challenge competition is to implement a solution to a real-world problem provided by the Grand Challenge organizers. The 2013 edition of the Grand Challenge focuses on real-time, event-based sports analytics. The 2013 Grand Challenge data set was collected during a football match car- ried out at a Nuremberg Stadium in Germany and is com- plemented with a set of continuous analytical queries which provide detailed insight into the match statistics for both team managers and spectators.},
  Annote                   = {problem domain: sport analytics
queries: running performance, ball possesion, heat map, shot on goal},
  Doi                      = {10.1145/2488222.2488283},
  File                     = {:home/giacomo/Documents/research/mendeley library/The DEBS 2013 grand challenge. 2013. Mutschler, Ziekow, Jerzak.pdf:pdf},
  ISBN                     = {9781450317580},
  Keywords                 = {allow-,cep,data stream processing,debs,edge to team managers,event processing,goal of the analytical,on one hand side,queries is twofold,streaming,they provide the competitive},
  Mendeley-tags            = {data stream processing,debs},
  Url                      = {http://dl.acm.org/citation.cfm?doid=2488222.2488283}
}

@TechReport{IDCBigDataForecast20152019,
  Title                    = {{Worldwide Storage in Big Data Forecast, 2015-2019}},
  Author                   = {Nadkarni, Ashish and Iris, Feng and Laura, DuBois},
  Institution              = {IDC},
  Year                     = {2015},

  Keywords                 = {bigdata,forecast},
  Mendeley-tags            = {bigdata,forecast},
  Pages                    = {11},
  Url                      = {http://bit.ly/idc-bigdata-forecast-1519}
}

@Misc{OracleJNI,
  Title                    = {{Oracle Java Native Interface}},

  Author                   = {Oracle},
  HowPublished             = {http://bit.ly/oracle-jni},
  Year                     = {2015},

  Institution              = {Oracle},
  Keywords                 = {java,jni,oracle},
  Mendeley-tags            = {java,jni,oracle},
  Url                      = {http://bit.ly/oracle-jni}
}

@Book{Pettorossi2014,
  Title                    = {{Elements of Computability, Decidability, and Complexity}},
  Author                   = {Pettorossi, Alberto},
  Editor                   = {Aracne},
  Year                     = {2014},
  Edition                  = {4},

  ISBN                     = {978-8854867895},
  Keywords                 = {complexity,computability,decidability},
  Mendeley-tags            = {complexity,computability,decidability}
}

@Article{PhilipChen2014,
  Title                    = {{Data-intensive applications, challenges, techniques and technologies: A survey on Big Data}},
  Author                   = {{Philip Chen}, C. L. and Zhang, Chun Yang},
  Journal                  = {Information Sciences},
  Year                     = {2014},
  Pages                    = {314--347},
  Volume                   = {275},

  Abstract                 = {It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore's Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing. © 2014 Elsevier Inc. All rights reserved.},
  Doi                      = {10.1016/j.ins.2014.01.015},
  File                     = {:home/giacomo/Documents/research/mendeley library/Data-intensive applications, challenges, techniques and technologies A survey on Big Data. 2014. Philip Chen, Zhang.pdf:pdf},
  ISBN                     = {0020-0255},
  ISSN                     = {00200255},
  Keywords                 = {Big Data,Cloud computing,Data-intensive computing,Parallel and distributed computing,e-Science},
  Publisher                = {Elsevier Inc.},
  Url                      = {https://doi.org/10.1016/j.ins.2014.01.015}
}

@InProceedings{Pietzuch2006,
  Title                    = {{Network-Aware Operator Placement for Stream-Processing Systems}},
  Author                   = {Pietzuch, Peter and Ledlie, Jonathan and Shneidman, Jeffrey and Roussopoulos, Mema and Welsh, Matt and Seltzer, Margo},
  Booktitle                = {22nd International Conference on Data Engineering (ICDE'06)},
  Year                     = {2006},
  Pages                    = {49--49},
  Publisher                = {IEEE},
  Volume                   = {2006},

  Abstract                 = {To use their pool of resources efficiently, distributed stream-processing systems push query operators to nodes within the network. Currently, these operators, ranging from simple filters to custom business logic, are placed manually at intermediate nodes along the transmission path to meet application-specific performance goals. Determining placement locations is challenging because network and node conditions change over time and because streams may interact with each other, opening venues for reuse and repositioning of operators. This paper describes a stream-based overlay network (SBON), a layer between a stream-processing system and the physical network that manages operator placement for stream-processing systems. Our design is based on a cost space, an abstract representation of the network and on-going streams, which permits decentralized, large-scale multi-query optimization decisions. We present an evaluation of the SBON approach through simulation, experiments on PlanetLab, and an integration with Borealis, an existing stream-processing engine. Our results show that an SBON consistently improves network utilization, provides low stream latency, and enables dynamic optimization at low engineering cost.},
  Doi                      = {10.1109/ICDE.2006.105},
  File                     = {:home/giacomo/Documents/research/mendeley library/Network-Aware Operator Placement for Stream-Processing Systems. 2006. Pietzuch et al.pdf:pdf},
  ISBN                     = {0-7695-2570-9},
  ISSN                     = {10844627},
  Pmid                     = {19717620},
  Url                      = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1617417}
}

@Book{Ross2015,
  Title                    = {{Probabilit{\`{a}} e statistica per l'ingegneria e le scienze}},
  Author                   = {Ross, Sheldon M. and Morandin, Francesco},
  Editor                   = {{Apogeo Education}},
  Year                     = {2015},

  ISBN                     = {978-8891609946},
  Pages                    = {638}
}

@InProceedings{Schneider2013,
  Title                    = {{Tutorial: Stream Processing Optimizations}},
  Author                   = {Schneider, Scott and Hirzel, Martin and Gedik, Buğra},
  Booktitle                = {Proceedings of the 7th ACM international conference on Distributed event-based systems - DEBS '13},
  Year                     = {2013},

  Address                  = {New York, New York, USA},
  Pages                    = {249},
  Publisher                = {ACM Press},
  Series                   = {DEBS '13},

  Abstract                 = {This tutorial starts with a survey of optimizations for streaming applications. The survey is organized as a cat- alog that introduces uniform terminology and a common categorization of optimizations across disciplines, such as data management, programming languages, and operating systems. After this survey, the tutorial continues with a deep-dive into the fission optimization, which automatically transforms streaming applications for data-parallelism. Fission helps an application improve its throughput by taking advantage of multiple cores in a machine, or, in the case of a distributed streaming engine, multiple machines in a cluster. While the survey of optimizations covers a wide range of work from the literature, the in-depth discussion of fission relies more heavily on the presenters’ own research and experience in the area. The tutorial concludes with a discussion of open research challenges in the field of stream processing optimizations. Categories},
  Doi                      = {10.1145/2488222.2488268},
  File                     = {:home/giacomo/Documents/research/mendeley library/Tutorial Stream Processing Optimizations. 2013. Schneider, Hirzel, Gedik.pdf:pdf},
  ISBN                     = {9781450317580},
  Keywords                 = {data parallelism,fission,stream processing},
  Url                      = {http://dl.acm.org/citation.cfm?doid=2488222.2488268}
}

@Article{Schroeck2012,
  Title                    = {{Analytics: The real-world use of big data}},
  Author                   = {Schroeck, Michael and Shockley, Rebecca and Smart, Janet and Romero-Morales, Dolores and Tufano, Peter},
  Journal                  = {IBM Global Business Services Sa{\"{\i}}d Business School at the University of Oxford},
  Year                     = {2012},
  Pages                    = {1--20},

  Abstract                 = {“Big data” – which admittedly means many things to many people – is no longer confined to the realm of technology. Today it is a business priority, given its ability to profoundly affect commerce in the globally integrated economy. In addition to providing solutions to long-standing business challenges, big data inspires new ways to transform processes, organizations, entire industries and even society itself. Yet extensive media coverage makes it hard to distinguish hype from reality – what is really happening? Our newest research finds that organizations are using big data to target customer-centric outcomes, tap into internal data and build a better information ecosystem.},
  File                     = {:home/giacomo/Documents/research/mendeley library/Analytics The real-world use of big data. 2012. Schroeck et al.pdf:pdf}
}

@Misc{Scopus,
  Title                    = {{Big Data papers trends}},

  Author                   = {Scopus},
  HowPublished             = {http://bit.ly/scopus-home},
  Year                     = {2015},

  Institution              = {Scopus},
  Url                      = {http://bit.ly/scopus-home}
}

@Book{Serafini2009,
  Title                    = {{Ricerca Operativa}},
  Author                   = {Serafini, Paolo},
  Editor                   = {{Springer Verlag}},
  Publisher                = {Springer Verlag},
  Year                     = {2009},

  Address                  = {Udine, Italy},
  Edition                  = {1},

  ISBN                     = {978-8847008458},
  Pages                    = {551}
}

@Article{Spring2004,
  Title                    = {{Measuring ISP Topologies With Rocketfuel}},
  Author                   = {Spring, Neil and Mahajan, Ratul and Wetherall, David and Anderson, Thomas},
  Journal                  = {IEEE/ACM Transactions on Networking},
  Year                     = {2004},
  Number                   = {1},
  Pages                    = {2--16},
  Volume                   = {12},

  Abstract                 = { To date, realistic ISP topologies have not been accessible to the research community, leaving work that depends on topology on an uncertain footing. In this paper, we present new Internet mapping techniques that have enabled us to measure router-level ISP topologies. Our techniques reduce the number of required traces compared to a brute-force, all-to-all approach by three orders of magnitude without a significant loss in accuracy. They include the use of BGP routing tables to focus the measurements, the elimination of redundant measurements by exploiting properties of IP routing, better alias resolution, and the use of DNS to divide each map into POPs and backbone. We collect maps from ten diverse ISPs using our techniques, and find that our maps are substantially more complete than those of earlier Internet mapping efforts. We also report on properties of these maps, including the size of POPs, distribution of router outdegree, and the interdomain peering structure. As part of this work, we release our maps to the community.},
  Doi                      = {10.1109/TNET.2003.822655},
  File                     = {:home/giacomo/Documents/research/mendeley library/Measuring ISP Topologies With Rocketfuel. 2004. Spring et al.pdf:pdf},
  ISBN                     = {158113570X},
  ISSN                     = {10636692},
  Keywords                 = {Communication system operations and management,Internet,Measurement,Network reliability}
}

@Article{Stonebraker2005,
  Title                    = {{The 8 requirements of real-time stream processing}},
  Author                   = {Stonebraker, Michael and {\c{C}}etintemel, Ugur and Zdonik, Stan},
  Journal                  = {ACM SIGMOD Record},
  Year                     = {2005},
  Number                   = {4},
  Pages                    = {42--47},
  Volume                   = {34},

  Abstract                 = {Applications that require real-time processing of high-volume data steams are pushing the limits of traditional data processing infrastructures. These stream-based applications include market feed processing and electronic trading on Wall Street, network and infrastructure monitoring, fraud detection, and command and control in military environments. Furthermore, as the "sea change" caused by cheap micro-sensor technology takes hold, we expect to see everything of material significance on the planet get "sensor-tagged" and report its state or location in real time. This sensorization of the real world will lead to a "green field" of novel monitoring and control applications with high-volume and low-latency processing requirements.Recently, several technologies have emerged--including off-the-shelf stream processing engines--specifically to address the challenges of processing high-volume, real-time data without requiring the use of custom code. At the same time, some existing software technologies, such as main memory DBMSs and rule engines, are also being "repurposed" by marketing departments to address these applications.In this paper, we outline eight requirements that a system software should meet to excel at a variety of real-time stream processing applications. Our goal is to provide high-level guidance to information technologists so that they will know what to look for when evaluation alternative stream processing solutions. As such, this paper serves a purpose comparable to the requirements papers in relational DBMSs and on-line analytical processing. We also briefly review alternative system software technologies in the context of our requirements.The paper attempts to be vendor neutral, so no specific commercial products are mentioned.},
  Doi                      = {10.1145/1107499.1107504},
  File                     = {:home/giacomo/Documents/research/mendeley library/The 8 requirements of real-time stream processing. 2005. Stonebraker, {\c{C}}etintemel, Zdonik.pdf:pdf},
  ISSN                     = {01635808},
  Url                      = {http://cs.brown.edu/{~}ugur/8rulesSigRec.pdf}
}

@Article{Suthaharan2013,
  Title                    = {{Big Data Classification: Problems and Challenges in Network Intrusion Prediction with Machine Learning}},
  Author                   = {Suthaharan, Shan},
  Journal                  = {ACM SIGMETRICS Performance Evaluation Review},
  Year                     = {2014},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {70--73},
  Volume                   = {41},

  Abstract                 = {This paper focuses on the specific problem of Big Data classification of network intrusion traffic. It discusses the system challenges presented by the Big Data problems associated with network intrusion prediction. The prediction of a possible intrusion attack in a network requires continuous collection of traffic data and learning of their characteristics on the fly. The continuous collection of traffic data by the network leads to Big Data problems that are caused by the volume, variety and velocity properties of Big Data. The learning of the network characteristics requires machine learning techniques that capture global knowledge of the traffic patterns. The Big Data properties will lead to significant system challenges to implement machine learning frameworks. This paper discusses the problems and challenges in handling Big Data classification using geometric representation-learning techniques and the modern Big Data networking technologies. In particular this paper discusses the issues related to combining supervised learning techniques, representation-learning techniques, machine lifelong learning techniques and Big Data technologies (e.g. Hadoop, Hive and Cloud) for solving network traffic classification problems.},
  Doi                      = {10.1145/2627534.2627557},
  File                     = {:home/giacomo/Documents/research/mendeley library/Big Data Classification Problems and Challenges in Network Intrusion Prediction with Machine Learning. 2014. Suthaharan.pdf:pdf},
  ISSN                     = {01635999},
  Keywords                 = {Big Data,Hadoop distributed file systems,intrusion detection,machine learning},
  Url                      = {http://dl.acm.org/citation.cfm?doid=2627534.2627557}
}

@Misc{GoogleTrendsBigData,
  Title                    = {{'Big Data' and 'Apache Hadoop'}},

  Author                   = {Google Trends},
  HowPublished             = {http://bit.ly/google-trends-bigdata-hadoop},

  Institution              = {Google Trends},
  Keywords                 = {big data,google,hadoop,trends},
  Mendeley-tags            = {big data,google,hadoop,trends},
  Url                      = {http://bit.ly/google-trends-bigdata-hadoop}
}

@Book{Trevisan2002,
  Title                    = {{Lecture Notes on Computational Complexity}},
  Author                   = {Trevisan, Luca},
  Year                     = {2002},

  Address                  = {Berkeley, USA},
  Number                   = {9984703},

  Booktitle                = {Notes written in Fall},
  File                     = {:home/giacomo/Documents/research/mendeley library/Lecture Notes on Computational Complexity. 2002. Trevisan.pdf:pdf},
  Pages                    = {171},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.9877{\&}rep=rep1{\&}type=pdf}
}

@Misc{TwitterStreamingAPI,
  Title                    = {{Streamig API}},

  Author                   = {Twitter},
  HowPublished             = {http://bit.ly/twitter-streaming-api},
  Year                     = {2015},

  Booktitle                = {Developers API},
  Institution              = {Twitter},
  Keywords                 = {api,data stream processing,twitter},
  Mendeley-tags            = {api,data stream processing,twitter},
  Url                      = {http://bit.ly/twitter-streaming-api}
}

@Article{Universitat2015,
  Title                    = {{DEBS Grand Challenge : Real Time Data Analysis of Taxi Rides using StreamMine3G}},
  Author                   = {Universit{\"{a}}t, Technische and Dresden, Dresden},
  Pages                    = {269--276},

  Abstract                 = {In this paper, we present our approach for solving the DEBS Grand Challenge 2015 using StreamMine3G, a distributed, highly scalable, elastic and fault tolerant ESP system. We first provide an overview about the system architecture of StreamMine3G followed by a thorough description of our implementation for the two queries that provide continuously up-to-date information about (i) the top-k most frequently driven routes and (ii) most profitable areas. Novel aspects of our implementation include two self-balancing double linked list implementations to efficiently up- date and determine a top-k as well as a median from a set of samples. Furthermore, we present a solution that supports data partitioning which allows the application to scale without bounds while still guaranteeing semantic transparency through the deterministic processing approach offered by the StreamMine3G runtime. In our evaluation, we provide measurements that show that our system can scale horizontally as well as vertically and can process 13 kEvents/s on a sin- gle node which translates to a processing of 3.8 hours of real time data within a second and a latency under 1 ms.},
  File                     = {:home/giacomo/Documents/research/mendeley library/DEBS Grand Challenge Real Time Data Analysis of Taxi Rides using StreamMine3G. Unknown. Universit{\"{a}}t, Dresden.pdf:pdf},
  ISBN                     = {9781450332866},
  Keywords                 = {cep,complex event processing,esp,event stream processing,fault tol-,migration,scalability,state management}
}

@Misc{VCloudBigData,
  Title                    = {{Every day Big Data Statistics}},

  Author                   = {VCloudNews},
  HowPublished             = {http://bit.ly/vcloudnews-quintillions-bytes},
  Year                     = {2015},

  Institution              = {VCloudNews},
  Url                      = {http://bit.ly/vcloudnews-quintillions-bytes}
}

@Misc{WikiCAGR,
  Title                    = {{Compound Annual Growth Rate (CAGR)}},

  Author                   = {Wikipedia},
  HowPublished             = {http://bit.ly/scopus-home},

  Institution              = {Wikipedia},
  Keywords                 = {cagr,wikipedia},
  Mendeley-tags            = {cagr,wikipedia},
  Url                      = {http://bit.ly/wiki-cagr}
}

@InProceedings{Zhao2015,
  Title                    = {{Optimal Algorithm Design Of Center of Graph Based on IBM ILOG OPL Model}},
  Author                   = {Zhao, Lixin and Hu, Lishuan},
  Booktitle                = {International Conference on Information Engineering for Mechanics and Materials},
  Year                     = {2015},
  Pages                    = {960--964},

  Abstract                 = {In this paper, Optimization problem is introduced. Some computer tool-kits used to solve optimization problems are outlined with their respective strength and weakness. A state of the art optimization programming language-IBM ILOG OPL is presented. This high level OPL facilitates modeling and solving optimal problem in all application fields significantly. An IBM ILOG OPL model of the shortest-path problem is implemented. Then, with IBM ILOG OPL model of Shortest-path problem as building block, an algorithm of center of graph is illustrated through calling this building block but with different data in consecutive way.This algorithm can be implemented with IBM ILOG Script language.This approach enable engineers in all kinds of field to solve optimal problems since no underlying mechanism of optimization need to be grasped.},
  File                     = {:home/giacomo/Documents/research/mendeley library/Optimal Algorithm Design Of Center of Graph Based on IBM ILOG OPL Model. 2015. Zhao, Hu.pdf:pdf},
  Keywords                 = {Center of Graph,OPL,Shortest-path Problem}
}

